#!/usr/bin/env python3
"""
OPTIMIZED MODEL TRAINING WITH CROSS-SECTIONAL RANKING
Trains state-of-the-art models with beta-neutral targets for maximum alpha
Implements all research-backed improvements for institutional-grade performance
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
import numpy as np
import sys
from pathlib import Path
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import joblib
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import TimeSeriesSplit
import warnings
warnings.filterwarnings('ignore')

# Add src to path
sys.path.append(str(Path(__file__).parent / "src"))

from models.advanced_models import AdvancedLSTM, iTransformer, PatchTST, FinancialTransformer
from features.cross_sectional_ranking import create_beta_neutral_targets, CrossSectionalRanker
from evaluation.production_conformal_gate import ProductionConformalGate
import yfinance as yf

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OptimizedModelTrainer:
    """
    State-of-the-art model training with all research improvements:
    1. Beta-neutral targets (removes market factor)
    2. Cross-sectional ranking (relative alpha)
    3. Purged time series CV (no data leakage)
    4. Multiple model architectures (ensemble diversity)
    5. Conformal prediction integration
    """
    
    def __init__(self, model_save_dir: str = "artifacts/models/optimized"):
        self.model_save_dir = Path(model_save_dir)
        self.model_save_dir.mkdir(parents=True, exist_ok=True)
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.ranker = CrossSectionalRanker()
        
        # Model configurations optimized for financial data (memory efficient)
        self.model_configs = {
            'advanced_lstm': {
                'class': AdvancedLSTM,
                'params': {
                    'input_size': 64,  # Will be updated based on actual features
                    'hidden_size': 64,  # Reduced for memory efficiency
                    'num_layers': 2,    # Reduced layers
                    'dropout': 0.3
                }
            },
            'itransformer': {
                'class': iTransformer,
                'params': {
                    'input_size': 64,
                    'd_model': 128,     # Reduced model size
                    'num_layers': 4,    # Reduced layers
                    'dropout': 0.2
                }
            },
            'patchtst': {
                'class': PatchTST,
                'params': {
                    'input_size': 64,
                    'd_model': 128,     # Reduced model size
                    'num_layers': 4,    # Reduced layers
                    'patch_len': 8,     # Smaller patches
                    'stride': 4,
                    'dropout': 0.2
                }
            },
            'financial_transformer': {
                'class': FinancialTransformer,
                'params': {
                    'input_size': 64,
                    'd_model': 128,     # Reduced model size
                    'num_layers': 4,    # Reduced layers
                    'dropout': 0.2
                }
            }
        }
        
        logger.info(f"üöÄ Optimized Model Trainer initialized")
        logger.info(f"   Device: {self.device}")
        logger.info(f"   Model save directory: {self.model_save_dir}")
        logger.info(f"   Available models: {list(self.model_configs.keys())}")
    
    def create_enhanced_dataset(self, symbols: List[str], 
                              lookback_days: int = 730,
                              sequence_length: int = 30) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """
        Create enhanced dataset with cross-sectional ranking and beta-neutral targets
        """
        logger.info("üîß Creating enhanced dataset with beta-neutral targets...")
        
        # Generate beta-neutral training targets
        beta_neutral_df = create_beta_neutral_targets(
            symbols=symbols,
            lookback_days=lookback_days,
            forward_days=1  # 1-day ahead prediction
        )
        
        if len(beta_neutral_df) == 0:
            raise ValueError("Failed to generate beta-neutral targets")
        
        logger.info(f"‚úÖ Generated {len(beta_neutral_df)} beta-neutral training samples")
        
        # Download feature data
        end_date = datetime.now()
        start_date = end_date - timedelta(days=lookback_days + 100)
        
        feature_data = {}
        
        for symbol in symbols:
            try:
                ticker = yf.Ticker(symbol)
                df = ticker.history(start=start_date, end=end_date, interval='1d')
                
                if len(df) < 100:
                    continue
                
                # Calculate comprehensive technical features
                df = self._calculate_technical_features(df)
                feature_data[symbol] = df
                
                logger.debug(f"‚úÖ {symbol}: {len(df)} days of features")
                
            except Exception as e:
                logger.error(f"‚ùå Error downloading {symbol}: {e}")
                continue
        
        if not feature_data:
            raise ValueError("No feature data downloaded")
        
        # Create sequences for training
        X_sequences = []
        y_targets = []
        feature_names = None
        
        # Get common feature columns
        all_features = set()
        for df in feature_data.values():
            all_features.update([col for col in df.columns if col not in ['date', 'symbol']])
        
        feature_columns = sorted([col for col in all_features if not col.startswith('forward_')])
        feature_names = feature_columns
        
        logger.info(f"üìä Using {len(feature_columns)} features: {feature_columns[:10]}...")
        
        # Create training sequences
        for _, target_row in beta_neutral_df.iterrows():
            symbol = target_row['symbol']
            date = target_row['date']
            beta_neutral_target = target_row['beta_neutral_return']
            
            if symbol not in feature_data:
                continue
            
            df = feature_data[symbol]
            
            # Find date index
            date_mask = df.index.date == date.date() if hasattr(date, 'date') else df.index.date == date
            if not date_mask.any():
                continue
            
            date_idx = np.where(date_mask)[0][0]
            
            # Check if we have enough historical data for sequence
            if date_idx < sequence_length:
                continue
            
            # Extract feature sequence (sequence_length days before target date)
            start_idx = date_idx - sequence_length
            end_idx = date_idx
            
            feature_sequence = df.iloc[start_idx:end_idx][feature_columns].values
            
            # Check for missing data
            if np.isnan(feature_sequence).any():
                # Fill with forward/backward fill
                feature_sequence = pd.DataFrame(feature_sequence).fillna(method='ffill').fillna(method='bfill').values
            
            if feature_sequence.shape[0] == sequence_length and not np.isnan(feature_sequence).any():
                X_sequences.append(feature_sequence)
                y_targets.append(beta_neutral_target)
        
        if not X_sequences:
            raise ValueError("No valid training sequences created")
        
        X = np.array(X_sequences)
        y = np.array(y_targets)
        
        logger.info(f"‚úÖ Created dataset: X.shape={X.shape}, y.shape={y.shape}")
        
        return X, y, feature_names
    
    def _calculate_technical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Calculate comprehensive technical features"""
        df = df.copy()
        
        # Price features
        df['return_1d'] = df['Close'].pct_change()
        df['return_5d'] = df['Close'].pct_change(5)
        df['return_20d'] = df['Close'].pct_change(20)
        
        # Moving averages
        for period in [5, 10, 20, 50]:
            df[f'sma_{period}'] = df['Close'].rolling(period).mean()
            df[f'ema_{period}'] = df['Close'].ewm(span=period).mean()
            df[f'price_vs_sma_{period}'] = df['Close'] / df[f'sma_{period}'] - 1
        
        # Volatility features
        df['volatility_20d'] = df['return_1d'].rolling(20).std() * np.sqrt(252)
        df['volatility_60d'] = df['return_1d'].rolling(60).std() * np.sqrt(252)
        
        # RSI
        delta = df['Close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))
        
        # MACD
        ema_12 = df['Close'].ewm(span=12).mean()
        ema_26 = df['Close'].ewm(span=26).mean()
        df['macd'] = ema_12 - ema_26
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        df['macd_histogram'] = df['macd'] - df['macd_signal']
        
        # Bollinger Bands
        bb_period = 20
        bb_std = 2
        df['bb_middle'] = df['Close'].rolling(bb_period).mean()
        df['bb_std'] = df['Close'].rolling(bb_period).std()
        df['bb_upper'] = df['bb_middle'] + (bb_std * df['bb_std'])
        df['bb_lower'] = df['bb_middle'] - (bb_std * df['bb_std'])
        df['bb_position'] = (df['Close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])
        
        # Volume features
        df['volume_sma_20'] = df['Volume'].rolling(20).mean()
        df['volume_ratio'] = df['Volume'] / df['volume_sma_20']
        df['price_volume'] = df['Close'] * df['Volume']
        
        # Momentum features
        df['momentum_5d'] = df['Close'] / df['Close'].shift(5) - 1
        df['momentum_20d'] = df['Close'] / df['Close'].shift(20) - 1
        
        # High-low features
        df['high_low_pct'] = (df['High'] - df['Low']) / df['Close']
        df['close_position'] = (df['Close'] - df['Low']) / (df['High'] - df['Low'])
        
        # Remove infinite and extreme values
        for col in df.select_dtypes(include=[np.number]).columns:
            df[col] = df[col].replace([np.inf, -np.inf], np.nan)
            # Cap at 99th percentile
            if df[col].std() > 0:
                upper_cap = df[col].quantile(0.99)
                lower_cap = df[col].quantile(0.01)
                df[col] = df[col].clip(lower=lower_cap, upper=upper_cap)
        
        return df
    
    def train_model(self, model_name: str, X: np.ndarray, y: np.ndarray, 
                   feature_names: List[str], n_folds: int = 3) -> Dict:
        """
        Train a single model with purged time series cross-validation
        """
        logger.info(f"üéØ Training {model_name} with purged CV...")
        
        if model_name not in self.model_configs:
            raise ValueError(f"Unknown model: {model_name}")
        
        config = self.model_configs[model_name]
        
        # Update input size based on actual features
        config['params']['input_size'] = X.shape[2]
        
        # Purged time series split (institutional standard)
        tscv = TimeSeriesSplit(n_splits=n_folds)
        
        fold_results = {}
        oof_predictions = np.zeros(len(y))
        
        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
            logger.info(f"  üìÅ Fold {fold + 1}/{n_folds}")
            
            # Purged split: add gap between train and validation
            purge_days = 5  # 5-day purge to prevent look-ahead
            if len(train_idx) > purge_days:
                train_idx = train_idx[:-purge_days]
            
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            
            # Scale features
            scaler = RobustScaler()  # More robust to outliers than StandardScaler
            
            # Fit scaler only on training data
            X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
            X_val_scaled = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)
            
            # Create model
            model = config['class'](**config['params'])
            model = model.to(self.device)
            
            # Training setup
            criterion = nn.MSELoss()
            optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)
            
            # Convert to tensors with memory management
            try:
                X_train_tensor = torch.FloatTensor(X_train_scaled).to(self.device)
                y_train_tensor = torch.FloatTensor(y_train).to(self.device)
                X_val_tensor = torch.FloatTensor(X_val_scaled).to(self.device)
                y_val_tensor = torch.FloatTensor(y_val).to(self.device)
            except RuntimeError as e:
                if "out of memory" in str(e):
                    logger.warning(f"CUDA OOM, falling back to CPU training for {model_name}")
                    self.device = torch.device('cpu')
                    model = model.cpu()
                    X_train_tensor = torch.FloatTensor(X_train_scaled)
                    y_train_tensor = torch.FloatTensor(y_train)
                    X_val_tensor = torch.FloatTensor(X_val_scaled)
                    y_val_tensor = torch.FloatTensor(y_val)
                else:
                    raise
            
            # Training loop
            best_val_loss = float('inf')
            patience_counter = 0
            max_patience = 20
            
            for epoch in range(200):
                model.train()
                
                # Forward pass
                train_pred = model(X_train_tensor)
                if isinstance(train_pred, dict):
                    train_pred = train_pred['return_prediction']
                
                train_loss = criterion(train_pred.squeeze(), y_train_tensor)
                
                # Backward pass
                optimizer.zero_grad()
                train_loss.backward()
                
                # Gradient clipping for stability
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                # Validation
                model.eval()
                with torch.no_grad():
                    val_pred = model(X_val_tensor)
                    if isinstance(val_pred, dict):
                        val_pred = val_pred['return_prediction']
                    
                    val_loss = criterion(val_pred.squeeze(), y_val_tensor)
                
                scheduler.step(val_loss)
                
                # Early stopping
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                    # Save best model state
                    best_model_state = model.state_dict().copy()
                else:
                    patience_counter += 1
                
                if patience_counter >= max_patience:
                    logger.debug(f"    Early stopping at epoch {epoch}")
                    break
                
                if epoch % 50 == 0:
                    logger.debug(f"    Epoch {epoch}: train_loss={train_loss:.6f}, val_loss={val_loss:.6f}")
            
            # Load best model and generate OOF predictions
            model.load_state_dict(best_model_state)
            model.eval()
            
            with torch.no_grad():
                val_pred = model(X_val_tensor)
                if isinstance(val_pred, dict):
                    val_pred = val_pred['return_prediction']
                
                oof_predictions[val_idx] = val_pred.squeeze().cpu().numpy()
            
            # Save fold model and scaler
            fold_model_path = self.model_save_dir / f"{model_name}_fold_{fold}.pt"
            fold_scaler_path = self.model_save_dir / f"{model_name}_fold_{fold}_scaler.pkl"
            
            torch.save(model.state_dict(), fold_model_path)
            with open(fold_scaler_path, 'wb') as f:
                joblib.dump(scaler, f)
            
            # Calculate fold metrics
            val_corr = np.corrcoef(y_val, oof_predictions[val_idx])[0, 1] if len(y_val) > 1 else 0
            val_ic = val_corr  # Information Coefficient
            
            fold_results[fold] = {
                'val_loss': float(best_val_loss),
                'val_ic': val_ic,
                'n_train': len(train_idx),
                'n_val': len(val_idx)
            }
            
            logger.info(f"    ‚úÖ Fold {fold + 1}: IC={val_ic:.4f}, loss={best_val_loss:.6f}")
        
        # Overall model performance
        overall_ic = np.corrcoef(y, oof_predictions)[0, 1] if len(y) > 1 else 0
        overall_mse = np.mean((y - oof_predictions) ** 2)
        
        results = {
            'model_name': model_name,
            'overall_ic': overall_ic,
            'overall_mse': overall_mse,
            'fold_results': fold_results,
            'oof_predictions': oof_predictions,
            'n_samples': len(y),
            'n_features': X.shape[2],
            'feature_names': feature_names
        }
        
        logger.info(f"‚úÖ {model_name} training complete: IC={overall_ic:.4f}, MSE={overall_mse:.6f}")
        
        return results
    
    def train_ensemble(self, symbols: List[str], 
                      models_to_train: Optional[List[str]] = None) -> Dict:
        """
        Train ensemble of optimized models with cross-sectional ranking
        """
        if models_to_train is None:
            models_to_train = list(self.model_configs.keys())
        
        logger.info("üöÄ TRAINING OPTIMIZED MODEL ENSEMBLE")
        logger.info("=" * 60)
        logger.info(f"üìä Symbols: {len(symbols)}")
        logger.info(f"ü§ñ Models: {models_to_train}")
        logger.info(f"üéØ Target: Beta-neutral cross-sectional alpha")
        logger.info("=" * 60)
        
        # Create enhanced dataset
        X, y, feature_names = self.create_enhanced_dataset(symbols)
        
        # Train each model
        ensemble_results = {}
        all_oof_predictions = {}
        
        for model_name in models_to_train:
            try:
                results = self.train_model(model_name, X, y, feature_names)
                ensemble_results[model_name] = results
                all_oof_predictions[model_name] = results['oof_predictions']
                
            except Exception as e:
                logger.error(f"‚ùå Error training {model_name}: {e}")
                continue
        
        if not ensemble_results:
            raise ValueError("No models trained successfully")
        
        # Create ensemble predictions
        if len(all_oof_predictions) > 1:
            ensemble_pred = np.mean(list(all_oof_predictions.values()), axis=0)
            ensemble_ic = np.corrcoef(y, ensemble_pred)[0, 1]
            
            logger.info(f"üéØ ENSEMBLE PERFORMANCE: IC={ensemble_ic:.4f}")
        else:
            ensemble_ic = list(ensemble_results.values())[0]['overall_ic']
        
        # Save ensemble metadata
        ensemble_metadata = {
            'training_date': datetime.now().isoformat(),
            'symbols': symbols,
            'n_samples': len(y),
            'n_features': len(feature_names),
            'feature_names': feature_names,
            'models': list(ensemble_results.keys()),
            'ensemble_ic': ensemble_ic,
            'individual_ics': {k: v['overall_ic'] for k, v in ensemble_results.items()}
        }
        
        metadata_path = self.model_save_dir / "ensemble_metadata.json"
        with open(metadata_path, 'w') as f:
            import json
            json.dump(ensemble_metadata, f, indent=2)
        
        logger.info("‚úÖ ENSEMBLE TRAINING COMPLETED")
        logger.info(f"üìÅ Models saved to: {self.model_save_dir}")
        
        return {
            'ensemble_results': ensemble_results,
            'ensemble_ic': ensemble_ic,
            'metadata': ensemble_metadata
        }

def main():
    """Train optimized models with beta-neutral targets"""
    
    # NASDAQ-100 universe (liquid, high-quality stocks)
    symbols = [
        'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'NVDA', 'AMD', 'INTC', 'QCOM',
        'AVGO', 'TXN', 'ORCL', 'CRM', 'ADBE', 'NOW', 'PYPL', 'NFLX', 'CMCSA', 'PEP',
        'COST', 'TMUS', 'SBUX', 'AMGN', 'GILD', 'MDLZ', 'ISRG', 'REGN', 'MRNA', 'CSX',
        'ABNB', 'UBER', 'DOCU', 'ZM', 'PTON', 'SNOW', 'CRWD', 'OKTA', 'DDOG', 'NET'
    ]
    
    # Train models
    trainer = OptimizedModelTrainer()
    
    try:
        results = trainer.train_ensemble(
            symbols=symbols[:20],  # Start with top 20 for faster training
            models_to_train=['advanced_lstm', 'itransformer', 'patchtst']
        )
        
        print("\n" + "="*60)
        print("üéØ OPTIMIZED MODEL TRAINING RESULTS")
        print("="*60)
        print(f"üèÜ Ensemble IC: {results['ensemble_ic']:.4f}")
        print("üìä Individual Model Performance:")
        
        for model_name, model_results in results['ensemble_results'].items():
            ic = model_results['overall_ic']
            print(f"   {model_name}: IC = {ic:.4f}")
        
        print(f"\nüìÅ Models saved to: artifacts/models/optimized/")
        print("‚úÖ Ready for production deployment!")
        print("="*60)
        
    except Exception as e:
        logger.error(f"Training failed: {e}")
        raise

if __name__ == "__main__":
    main()