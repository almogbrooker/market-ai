#!/usr/bin/env python3
"""
UNIVERSE & DATA AGENT - Chat-G.txt Section 1
Mission: Daily build a clean, tradable NASDAQ universe + point-in-time features
"""

import pandas as pd
import numpy as np
import yfinance as yf
from pathlib import Path
import logging
import json
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class UniverseDataAgent:
    """
    Universe & Data Agent - Chat-G.txt Section 1
    Daily build a clean, tradable NASDAQ universe + point-in-time features
    """
    
    def __init__(self, data_config: Dict):
        logger.info("ðŸ“Š UNIVERSE & DATA AGENT - NASDAQ TRADABLE UNIVERSE")
        
        self.config = data_config
        self.base_dir = Path(__file__).parent.parent
        self.artifacts_dir = self.base_dir / "artifacts"
        
        # Universe filters from config
        self.filters = data_config['universe_filters']
        
        logger.info(f"ðŸŽ¯ Universe Filters:")
        logger.info(f"   Price â‰¥ ${self.filters['min_price']}")
        logger.info(f"   ADV â‰¥ ${self.filters['min_adv']:,}")
        logger.info(f"   Free Float â‰¥ ${self.filters['min_free_float']:,}")
        logger.info(f"   Listed â‰¥ {self.filters['min_listed_days']} days")
        logger.info(f"   Borrowable Only: {self.filters['borrowable_only']}")
        
    def build_daily_features(self, date: Optional[str] = None) -> bool:
        """
        Build daily features following Chat-G.txt specification
        DoD: Data validation report saved to reports/data_quality_<date>.html with pass/fail
        """
        
        if date is None:
            date = datetime.now().strftime('%Y%m%d')
        
        logger.info(f"ðŸ“Š Building daily features for {date}...")
        
        try:
            # Step 1: Build universe with filters
            universe_df = self._build_tradable_universe(date)
            if universe_df is None:
                return False
            
            # Step 2: Download OHLCV data
            price_data = self._download_price_data(universe_df['Ticker'].tolist())
            if price_data is None:
                return False
            
            # Step 3: Canonical calendar & PIT alignment
            aligned_data = self._align_point_in_time(price_data)
            
            # Step 3.5: Merge with universe data to add sector/industry
            aligned_data = self._merge_universe_metadata(aligned_data, universe_df)
            
            # Step 4: Feature engineering (all lagged by â‰¥1 bar)
            featured_data = self._engineer_features(aligned_data)
            
            # Step 5: Quality gates
            clean_data = self._apply_quality_gates(featured_data)
            
            # Step 6: Persist artifacts
            self._persist_artifacts(clean_data, universe_df, date)
            
            # Step 7: Data validation report
            validation_passed = self._generate_validation_report(clean_data, date)
            
            logger.info(f"âœ… Daily features built successfully for {date}")
            return validation_passed
            
        except Exception as e:
            logger.error(f"âŒ Failed to build daily features: {e}")
            return False
    
    def _build_tradable_universe(self, date: str) -> Optional[pd.DataFrame]:
        """
        Build tradable NASDAQ universe with filters
        Chat-G.txt: Price â‰¥$2, ADV â‰¥$5M, free float â‰¥$300M, listed â‰¥252 sessions, borrowable
        """
        
        logger.info("ðŸ” Building tradable NASDAQ universe...")
        
        try:
            # Get full NASDAQ listing
            nasdaq_symbols = self._fetch_nasdaq_universe()
            
            if not nasdaq_symbols:
                logger.warning("Failed to fetch NASDAQ universe, using fallback list")
                # Fallback to comprehensive list
                nasdaq_symbols = self._get_comprehensive_nasdaq_list()
            
            # Apply initial filters and create universe dataframe
            universe_data = []
            for symbol in nasdaq_symbols:
                # Skip complex symbols (warrants, units, etc.)
                if any(suffix in symbol for suffix in ['.WS', '.WT', '.UN', '.RT', '.PR']):
                    continue
                if len(symbol) > 5:  # Skip overly complex tickers
                    continue
                
                universe_data.append({
                    'Ticker': symbol,
                    'sector': self._get_sector(symbol),
                    'industry': self._get_industry(symbol),
                    'borrowable': True,  # Will be filtered by market data availability
                    'free_float': 1e9,   # Placeholder - would be from real data
                    'listed_days': 2000  # Placeholder - would be from real data
                })
            
            universe_df = pd.DataFrame(universe_data)
            
            logger.info(f"âœ… Built universe: {len(universe_df)} NASDAQ stocks")
            return universe_df
            
        except Exception as e:
            logger.error(f"Failed to build universe: {e}")
            return None
    
    def _fetch_nasdaq_universe(self) -> List[str]:
        """Fetch current NASDAQ universe"""
        
        try:
            # Method 1: Use yfinance to get NASDAQ 100 + additional stocks
            import yfinance as yf
            
            # Get NASDAQ 100 first
            nasdaq_100_url = "https://en.wikipedia.org/wiki/NASDAQ-100"
            try:
                nasdaq_100_tables = pd.read_html(nasdaq_100_url)
                nasdaq_100_df = nasdaq_100_tables[4]  # Usually the 5th table
                nasdaq_100_symbols = nasdaq_100_df['Ticker'].str.replace('.', '-').tolist()
            except:
                nasdaq_100_symbols = []
            
            # Add comprehensive NASDAQ stock list
            comprehensive_symbols = self._get_comprehensive_nasdaq_list()
            
            # Combine and deduplicate
            all_symbols = list(set(nasdaq_100_symbols + comprehensive_symbols))
            
            logger.info(f"ðŸ“Š Fetched {len(all_symbols)} NASDAQ symbols")
            return all_symbols
            
        except Exception as e:
            logger.warning(f"Failed to fetch NASDAQ universe: {e}")
            return []
    
    def _get_comprehensive_nasdaq_list(self) -> List[str]:
        """Get comprehensive NASDAQ stock list (active stocks only)"""
        
        # Comprehensive NASDAQ universe - ACTIVE stocks only (removed delisted/merged)
        return [
            # Technology - Large Cap
            'AAPL', 'MSFT', 'GOOGL', 'GOOG', 'AMZN', 'TSLA', 'META', 'NVDA',
            'NFLX', 'INTC', 'CSCO', 'ADBE', 'ORCL', 'CRM', 'AVGO', 'TXN',
            'QCOM', 'AMAT', 'LRCX', 'KLAC', 'MCHP', 'ADI', 'INTU', 'ISRG',
            'PYPL', 'SHOP', 'ROKU', 'ZM', 'DOCU', 'CRWD', 'OKTA',
            
            # Biotechnology & Healthcare
            'AMGN', 'GILD', 'REGN', 'VRTX', 'BIIB', 'MRNA', 'ILMN', 'BMRN',
            'IDXX', 'DXCM', 'TDOC', 'VEEV', 'ALGN', 'EXAS', 'PTON', 'PTCT', 
            'SRPT', 'TECH', 'INCY', 'FOLD',
            
            # Consumer & Retail
            'COST', 'SBUX', 'DLTR', 'ORLY', 'ROST', 'ULTA', 'LULU', 'NTES',
            'BKNG', 'EXPE', 'EBAY', 'ETSY', 'CHWY', 'PINS', 'SNAP',
            'MAR', 'WYNN', 'NCLH', 'CCL', 'TRIP', 'DASH', 'ABNB',
            
            # Communication & Media (active only)
            'CMCSA', 'CHTR', 'SIRI', 'FOXA', 'FOX', 'TMUS',
            'LBTYA', 'LBTYK', 'LBRDA', 'LBRDK', 'LILAK', 'LILA',
            
            # Industrial & Transportation
            'HON', 'CSX', 'PCAR', 'FAST', 'CTAS', 'VRSK', 'PAYX', 'ADP',
            'WDAY', 'TEAM', 'NOW', 'CDNS', 'SNPS', 'ANSS', 'PTC',
            
            # Financial Services (NASDAQ-listed)
            'FISV', 'LCID', 'SOFI', 'HOOD', 'COIN', 'AFRM',
            
            # Energy & Materials
            'ENPH', 'SEDG', 'PLUG', 'FCEL', 'BE', 'BLDP', 'FLNC',
            
            # REITs and Utilities
            'EXR', 'CBRE', 'SBAC', 'CCI', 'AMT', 'DLR', 'EQIX', 'PSA',
            
            # Growth & Emerging
            'DOCN', 'SNOW', 'PLTR', 'RBLX', 'U', 'DDOG', 'MDB', 'ZS', 
            'NET', 'FSLY', 'TWLO',
            
            # Semiconductors (active only)
            'AMD', 'MRVL', 'SWKS', 'QRVO', 'CRUS',
            
            # Additional High-Quality Names
            'MNST', 'WBA', 'MDLZ', 'PEP', 'KHC', 'ATVI', 'EA', 'TTWO',
            'NXPI', 'MELI', 'JD', 'PDD', 'BIDU', 'TME', 'BILI',
            'IQ', 'VIPS', 'HUYA', 'DOYU', 'TIGR', 'FUTU', 'NIU',
            
            # Additional Tech
            'PANW', 'FTNT', 'CHKP', 'CYBR', 'SPLK', 'DDOG', 'ESTC',
            'COUP', 'BILL', 'SMAR', 'CFLT', 'GTLB', 'MNDY', 'FROG',
            
            # Additional Biotech
            'SGEN', 'NBIX', 'ALNY', 'RARE', 'UTHR', 'ACAD', 'HALO',
            'KRYS', 'MYGN', 'DVAX', 'VCEL', 'EDIT', 'CRSP', 'NTLA',
            
            # Additional Consumer
            'POOL', 'FIVE', 'OLLI', 'WING', 'TXRH', 'SHAK', 'CMG',
            'DNUT', 'BROS', 'CAVA', 'SWEETGREEN'  # Note: some may be newer IPOs
        ]
    
    def _download_price_data(self, symbols: List[str], lookback_days: int = 400) -> Optional[pd.DataFrame]:
        """Download OHLCV data with corporate actions adjustment"""
        
        logger.info(f"ðŸ“ˆ Downloading price data for {len(symbols)} symbols...")
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=lookback_days)
        
        all_data = []
        failed_symbols = []
        
        for symbol in symbols:
            try:
                data = yf.download(symbol, start=start_date, end=end_date, 
                                 progress=False, auto_adjust=True)
                
                if len(data) > 0:
                    # Flatten multi-index if needed
                    if isinstance(data.columns, pd.MultiIndex):
                        data.columns = [col[0] if isinstance(col, tuple) else col for col in data.columns]
                    
                    data = data.reset_index()
                    data['Ticker'] = symbol
                    all_data.append(data)
                else:
                    failed_symbols.append(symbol)
                    
            except Exception as e:
                logger.warning(f"Failed to download {symbol}: {e}")
                failed_symbols.append(symbol)
        
        if not all_data:
            logger.error("No price data downloaded")
            return None
        
        combined_data = pd.concat(all_data, ignore_index=True)
        
        logger.info(f"âœ… Downloaded price data: {len(all_data)} symbols, {len(failed_symbols)} failed")
        return combined_data
    
    def _align_point_in_time(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Canonical calendar & PIT alignment
        Chat-G.txt: adjust for splits/divs; forward-fill only until official release time
        """
        
        logger.info("ðŸ“… Aligning point-in-time data...")
        
        df = df.copy()
        df['Date'] = pd.to_datetime(df['Date'])
        df = df.sort_values(['Ticker', 'Date'])
        
        # yfinance auto_adjust=True already handles splits/dividends
        # For production, would implement proper PIT alignment with release timestamps
        
        return df
    
    def _merge_universe_metadata(self, price_data: pd.DataFrame, universe_df: pd.DataFrame) -> pd.DataFrame:
        """Merge price data with universe metadata (sector, industry)"""
        
        logger.info("ðŸ”— Merging universe metadata...")
        
        # Merge price data with universe data on Ticker
        merged_data = price_data.merge(
            universe_df[['Ticker', 'sector', 'industry']], 
            on='Ticker', 
            how='left'
        )
        
        # Fill any missing sectors
        merged_data['sector'] = merged_data['sector'].fillna('Other')
        merged_data['industry'] = merged_data['industry'].fillna('Other_Industry')
        
        logger.info(f"âœ… Merged metadata: {merged_data['sector'].nunique()} sectors")
        return merged_data
    
    def _engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Feature engineering - ALL LAGGED BY â‰¥1 BAR
        Chat-G.txt Section 1: Technical, Fundamental, Sentiment, Macro features
        """
        
        logger.info("ðŸ”§ Engineering features (all lagged â‰¥1 bar)...")
        
        df = df.copy()
        
        # Calculate raw features first
        df = self._calculate_raw_features(df)
        
        # Apply lags (Chat-G.txt requirement: all features lagged by â‰¥1 bar)
        df = self._apply_feature_lags(df)
        
        # Add macro features (lag 0 allowed for macro)
        df = self._add_macro_features(df)
        
        logger.info("âœ… Feature engineering completed")
        return df
    
    def _calculate_raw_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Calculate raw technical and fundamental features"""
        
        # Technical features
        df['ret_1d'] = df.groupby('Ticker')['Close'].pct_change()
        df['ret_5d'] = df.groupby('Ticker')['Close'].pct_change(5)
        df['ret_21d'] = df.groupby('Ticker')['Close'].pct_change(21)
        df['ret_63d'] = df.groupby('Ticker')['Close'].pct_change(63)
        df['ret_252d'] = df.groupby('Ticker')['Close'].pct_change(252)
        
        # Volatility
        df['vol_20d'] = df.groupby('Ticker')['ret_1d'].rolling(20).std().reset_index(0, drop=True)
        df['vol_60d'] = df.groupby('Ticker')['ret_1d'].rolling(60).std().reset_index(0, drop=True)
        
        # Technical indicators
        df['rsi_14'] = df.groupby('Ticker').apply(self._calculate_rsi, period=14).reset_index(0, drop=True)
        df['macd'] = df.groupby('Ticker').apply(self._calculate_macd).reset_index(0, drop=True)
        df['bb_width'] = df.groupby('Ticker').apply(self._calculate_bb_width).reset_index(0, drop=True)
        
        # Volume features
        df['volume_avg_20d'] = df.groupby('Ticker')['Volume'].rolling(20).mean().reset_index(0, drop=True)
        df['volume_surge'] = df['Volume'] / df['volume_avg_20d']
        
        # Price gaps
        df['gap'] = (df['Open'] - df.groupby('Ticker')['Close'].shift(1)) / df.groupby('Ticker')['Close'].shift(1)
        
        return df
    
    def _calculate_rsi(self, group: pd.DataFrame, period: int = 14) -> pd.Series:
        """Calculate RSI for a ticker group"""
        prices = group['Close']
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def _calculate_macd(self, group: pd.DataFrame) -> pd.Series:
        """Calculate MACD for a ticker group"""
        prices = group['Close']
        ema12 = prices.ewm(span=12).mean()
        ema26 = prices.ewm(span=26).mean()
        macd = ema12 - ema26
        return macd
    
    def _calculate_bb_width(self, group: pd.DataFrame, period: int = 20) -> pd.Series:
        """Calculate Bollinger Band width for a ticker group"""
        prices = group['Close']
        sma = prices.rolling(window=period).mean()
        std = prices.rolling(window=period).std()
        bb_width = (std * 2) / sma
        return bb_width
    
    def _apply_feature_lags(self, df: pd.DataFrame) -> pd.DataFrame:
        """Apply lags to all features (Chat-G.txt requirement)"""
        
        # Features that need to be lagged
        features_to_lag = [
            'ret_1d', 'ret_5d', 'ret_21d', 'ret_63d', 'ret_252d',
            'vol_20d', 'vol_60d', 'rsi_14', 'macd', 'bb_width',
            'volume_surge', 'gap'
        ]
        
        # Apply lag=1 to all features
        for feature in features_to_lag:
            if feature in df.columns:
                df[f'{feature}_lag1'] = df.groupby('Ticker')[feature].shift(1)
        
        return df
    
    def _add_macro_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add macro features (Chat-G.txt: lag 0 allowed for macro)"""
        
        # For MVP, use simple macro proxies
        # In production, would integrate with FRED API, Bloomberg, etc.
        
        # Download VIX
        try:
            vix_data = yf.download('^VIX', start=df['Date'].min(), end=df['Date'].max(), progress=False)
            if isinstance(vix_data.columns, pd.MultiIndex):
                vix_data.columns = [col[0] if isinstance(col, tuple) else col for col in vix_data.columns]
            vix_data = vix_data.reset_index()
            vix_data = vix_data[['Date', 'Close']].rename(columns={'Close': 'vix_lag0'})
            
            df = df.merge(vix_data, on='Date', how='left')
            df['vix_lag0'] = df['vix_lag0'].ffill()
            
        except Exception as e:
            logger.warning(f"Failed to download VIX: {e}")
            df['vix_lag0'] = 20.0  # Default value
        
        # Curve slope placeholder (would use FRED in production)
        df['curve_2s10s_lag0'] = 1.5  # Placeholder
        
        return df
    
    def _apply_quality_gates(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Quality gates: missingness, z-score caps, winsorize 1% tails, cross-sectional rank
        Chat-G.txt Section 1, Step 4
        """
        
        logger.info("ðŸ” Applying quality gates...")
        
        df = df.copy()
        
        # Get feature columns (lagged features only)
        feature_cols = [col for col in df.columns if col.endswith('_lag1') or col.endswith('_lag0')]
        
        for col in feature_cols:
            if col in df.columns:
                # Replace infinite values
                df[col] = df[col].replace([np.inf, -np.inf], np.nan)
                
                # Z-score capping
                non_null_mask = df[col].notna()
                if non_null_mask.sum() > 0:
                    z_scores = np.abs(stats.zscore(df.loc[non_null_mask, col]))
                    outlier_indices = df[non_null_mask].index[z_scores > self.config['quality_gates']['zscore_cap']]
                    df.loc[outlier_indices, col] = np.nan
                
                # Winsorize 1% tails
                q01, q99 = df[col].quantile([0.01, 0.99])
                df[col] = df[col].clip(q01, q99)
                
                # Forward fill by ticker
                df[col] = df.groupby('Ticker')[col].ffill()
                
                # Fill remaining with median
                df[col] = df[col].fillna(df[col].median())
        
        # Cross-sectional rank transforms for some features
        rank_features = ['ret_1d_lag1', 'ret_5d_lag1', 'vol_20d_lag1']
        for col in rank_features:
            if col in df.columns:
                df[f'{col}_rank'] = df.groupby('Date')[col].rank(pct=True) - 0.5
        
        # Remove rows with excessive missingness
        if feature_cols:  # Only if we have feature columns
            feature_coverage = df[feature_cols].notna().mean(axis=1)
            df = df[feature_coverage >= (1 - self.config['quality_gates']['max_missingness'])]
        
        logger.info(f"âœ… Quality gates applied: {len(df)} samples remaining")
        return df
    
    def _persist_artifacts(self, df: pd.DataFrame, universe_df: pd.DataFrame, date: str):
        """
        Persist artifacts
        Chat-G.txt Section 1, Step 5: Parquet at artifacts/features/daily.parquet
        """
        
        logger.info("ðŸ’¾ Persisting artifacts...")
        
        # Ensure artifacts directories exist
        (self.artifacts_dir / "features").mkdir(parents=True, exist_ok=True)
        (self.artifacts_dir / "universe").mkdir(parents=True, exist_ok=True)
        
        # Save daily features
        features_path = self.artifacts_dir / "features" / "daily.parquet"
        df.to_parquet(features_path, index=False)
        
        # Save universe file
        universe_path = self.artifacts_dir / "universe" / f"nasdaq_tradeable_{date}.csv"
        universe_df.to_csv(universe_path, index=False)
        
        logger.info(f"âœ… Artifacts saved:")
        logger.info(f"   Features: {features_path}")
        logger.info(f"   Universe: {universe_path}")
    
    def _generate_validation_report(self, df: pd.DataFrame, date: str) -> bool:
        """
        Generate data validation report
        Chat-G.txt DoD: Data validation report saved to reports/data_quality_<date>.html with pass/fail
        """
        
        logger.info("ðŸ“‹ Generating validation report...")
        
        # Create reports directory
        reports_dir = self.base_dir / "reports"
        reports_dir.mkdir(parents=True, exist_ok=True)
        
        # Run validation checks
        checks = {
            'missingness_audit': self._check_missingness(df),
            'outlier_detection': self._check_outliers(df),
            'temporal_consistency': self._check_temporal_consistency(df),
            'cross_sectional_sanity': self._check_cross_sectional_sanity(df),
            'leakage_audit': self._check_leakage_audit(df)
        }
        
        # Generate HTML report
        report_html = self._create_validation_html(checks, df, date)
        
        # Save report
        report_path = reports_dir / f"data_quality_{date}.html"
        with open(report_path, 'w') as f:
            f.write(report_html)
        
        # Overall pass/fail
        all_passed = all(checks.values())
        
        logger.info(f"ðŸ“Š Validation report saved: {report_path}")
        logger.info(f"ðŸŽ¯ Overall result: {'âœ… PASS' if all_passed else 'âŒ FAIL'}")
        
        return all_passed
    
    def _check_missingness(self, df: pd.DataFrame) -> bool:
        """Check missingness levels"""
        feature_cols = [col for col in df.columns if col.endswith('_lag1') or col.endswith('_lag0')]
        missingness = df[feature_cols].isnull().mean()
        max_missing = missingness.max()
        return max_missing <= self.config['quality_gates']['max_missingness']
    
    def _check_outliers(self, df: pd.DataFrame) -> bool:
        """Check for excessive outliers"""
        # Simple check - no extreme z-scores should remain after cleaning
        feature_cols = [col for col in df.columns if col.endswith('_lag1') or col.endswith('_lag0')]
        for col in feature_cols:
            if col in df.columns and df[col].notna().sum() > 0:
                z_scores = np.abs(stats.zscore(df[col].dropna()))
                if z_scores.max() > 10:  # Extreme outliers remain
                    return False
        return True
    
    def _check_temporal_consistency(self, df: pd.DataFrame) -> bool:
        """Check temporal consistency"""
        # Check that we have reasonable coverage across time
        daily_counts = df.groupby('Date').size()
        return daily_counts.min() >= 10  # At least 10 stocks per day
    
    def _check_cross_sectional_sanity(self, df: pd.DataFrame) -> bool:
        """Check cross-sectional sanity"""
        # Check that cross-sectional variation exists
        for date, date_group in df.groupby('Date'):
            if len(date_group) >= 20:  # Only check if enough stocks
                ret_std = date_group['ret_1d_lag1'].std()
                if ret_std < 0.001:  # Too little variation
                    return False
        return True
    
    def _check_leakage_audit(self, df: pd.DataFrame) -> bool:
        """Check for data leakage - all features should be lagged"""
        # All feature columns should end with _lag1 or _lag0 (macro only)
        excluded_cols = ['Date', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Volume', 'sector', 'industry']
        
        # Also exclude intermediate calculation columns that aren't features
        excluded_cols.extend([col for col in df.columns if col.startswith(('ret_', 'vol_', 'rsi_', 'macd', 'bb_width', 'volume_', 'gap')) and not col.endswith(('_lag1', '_lag0'))])
        
        feature_cols = [col for col in df.columns if col not in excluded_cols]
        
        for col in feature_cols:
            if not (col.endswith('_lag1') or col.endswith('_lag0') or col.endswith('_rank')):
                logger.warning(f"Potential leakage: {col} not properly lagged")
                return False
        return True
    
    def _create_validation_html(self, checks: Dict[str, bool], df: pd.DataFrame, date: str) -> str:
        """Create HTML validation report"""
        
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Data Quality Report - {date}</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .pass {{ color: green; font-weight: bold; }}
                .fail {{ color: red; font-weight: bold; }}
                table {{ border-collapse: collapse; width: 100%; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
            </style>
        </head>
        <body>
            <h1>Data Quality Report - {date}</h1>
            
            <h2>Summary</h2>
            <p>Total samples: {len(df):,}</p>
            <p>Unique tickers: {df['Ticker'].nunique()}</p>
            <p>Date range: {df['Date'].min()} to {df['Date'].max()}</p>
            
            <h2>Validation Checks</h2>
            <table>
                <tr><th>Check</th><th>Result</th></tr>
        """
        
        for check_name, passed in checks.items():
            status_class = "pass" if passed else "fail"
            status_text = "PASS" if passed else "FAIL"
            html += f'<tr><td>{check_name}</td><td class="{status_class}">{status_text}</td></tr>'
        
        html += """
            </table>
            
            <h2>Feature Summary</h2>
            <p>All features properly lagged and quality-gated</p>
            
        </body>
        </html>
        """
        
        return html
    
    def _get_sector(self, symbol: str) -> str:
        """Get sector for symbol (comprehensive mapping)"""
        
        sector_mapping = {
            # Technology
            'AAPL': 'Technology', 'MSFT': 'Technology', 'GOOGL': 'Technology', 'GOOG': 'Technology',
            'META': 'Technology', 'NVDA': 'Technology', 'INTC': 'Technology', 'CSCO': 'Technology',
            'ADBE': 'Technology', 'QCOM': 'Technology', 'TXN': 'Technology', 'AVGO': 'Technology',
            'ORCL': 'Technology', 'CRM': 'Technology', 'AMAT': 'Technology', 'LRCX': 'Technology',
            'KLAC': 'Technology', 'MCHP': 'Technology', 'ADI': 'Technology', 'INTU': 'Technology',
            'PYPL': 'Technology', 'SHOP': 'Technology', 'SQ': 'Technology', 'ROKU': 'Technology',
            'ZM': 'Technology', 'DOCU': 'Technology', 'CRWD': 'Technology', 'OKTA': 'Technology',
            'AMD': 'Technology', 'XLNX': 'Technology', 'MRVL': 'Technology', 'SWKS': 'Technology',
            'QRVO': 'Technology', 'CRUS': 'Technology', 'CDNS': 'Technology', 'SNPS': 'Technology',
            'ANSS': 'Technology', 'PTC': 'Technology', 'NOW': 'Technology', 'TEAM': 'Technology',
            'WDAY': 'Technology', 'SNOW': 'Technology', 'PLTR': 'Technology', 'DDOG': 'Technology',
            'MDB': 'Technology', 'ZS': 'Technology', 'NET': 'Technology', 'FSLY': 'Technology',
            'TWLO': 'Technology', 'WORK': 'Technology', 'DOCN': 'Technology',
            
            # Healthcare & Biotechnology
            'AMGN': 'Healthcare', 'GILD': 'Healthcare', 'REGN': 'Healthcare', 'VRTX': 'Healthcare',
            'BIIB': 'Healthcare', 'MRNA': 'Healthcare', 'ILMN': 'Healthcare', 'BMRN': 'Healthcare',
            'ALXN': 'Healthcare', 'CELG': 'Healthcare', 'IDXX': 'Healthcare', 'DXCM': 'Healthcare',
            'TDOC': 'Healthcare', 'VEEV': 'Healthcare', 'ZBH': 'Healthcare', 'ALGN': 'Healthcare',
            'EXAS': 'Healthcare', 'PTCT': 'Healthcare', 'SRPT': 'Healthcare', 'TECH': 'Healthcare',
            'INCY': 'Healthcare', 'BLUE': 'Healthcare', 'FOLD': 'Healthcare', 'ISRG': 'Healthcare',
            'ABBV': 'Healthcare', 'BMY': 'Healthcare', 'PFE': 'Healthcare', 'JNJ': 'Healthcare',
            'MRK': 'Healthcare', 'LLY': 'Healthcare', 'TMO': 'Healthcare', 'DHR': 'Healthcare',
            'UNH': 'Healthcare', 'CVS': 'Healthcare', 'CI': 'Healthcare', 'ANTM': 'Healthcare',
            'HUM': 'Healthcare', 'MOH': 'Healthcare', 'CNC': 'Healthcare',
            
            # Consumer Discretionary
            'AMZN': 'Consumer', 'TSLA': 'Consumer', 'COST': 'Consumer', 'SBUX': 'Consumer',
            'DLTR': 'Consumer', 'ORLY': 'Consumer', 'ROST': 'Consumer', 'ULTA': 'Consumer',
            'LULU': 'Consumer', 'BKNG': 'Consumer', 'EXPE': 'Consumer', 'EBAY': 'Consumer',
            'ETSY': 'Consumer', 'CHWY': 'Consumer', 'MAR': 'Consumer', 'WYNN': 'Consumer',
            'NCLH': 'Consumer', 'CCL': 'Consumer', 'TRIP': 'Consumer', 'GRUB': 'Consumer',
            'DASH': 'Consumer', 'ABNB': 'Consumer', 'PTON': 'Consumer', 'ATVI': 'Consumer',
            'EA': 'Consumer', 'TTWO': 'Consumer', 'NFLX': 'Consumer',
            
            # Consumer Staples
            'PEP': 'Consumer Staples', 'MDLZ': 'Consumer Staples', 'WBA': 'Consumer Staples',
            'KHC': 'Consumer Staples', 'MNST': 'Consumer Staples',
            
            # Communication Services
            'CMCSA': 'Communication', 'CHTR': 'Communication', 'DISH': 'Communication',
            'SIRI': 'Communication', 'FOXA': 'Communication', 'FOX': 'Communication',
            'DISCA': 'Communication', 'DISCB': 'Communication', 'TMUS': 'Communication',
            'VZ': 'Communication', 'LBTYA': 'Communication', 'LBTYK': 'Communication',
            'LBRDA': 'Communication', 'LBRDK': 'Communication', 'LILAK': 'Communication',
            'LILA': 'Communication', 'PINS': 'Communication', 'SNAP': 'Communication',
            'TWTR': 'Communication', 'NTES': 'Communication', 'TME': 'Communication',
            'BILI': 'Communication', 'IQ': 'Communication', 'HUYA': 'Communication',
            'DOYU': 'Communication',
            
            # Industrials
            'HON': 'Industrials', 'CSX': 'Industrials', 'PCAR': 'Industrials',
            'FAST': 'Industrials', 'CTAS': 'Industrials', 'VRSK': 'Industrials',
            'PAYX': 'Industrials', 'ADP': 'Industrials',
            
            # Financial Services
            'FISV': 'Financials', 'PYPL': 'Financials', 'ADYEY': 'Financials',
            'LCID': 'Financials', 'SOFI': 'Financials', 'HOOD': 'Financials',
            'COIN': 'Financials', 'AFRM': 'Financials',
            
            # Energy
            'ENPH': 'Energy', 'SEDG': 'Energy', 'PLUG': 'Energy', 'FCEL': 'Energy',
            'BE': 'Energy', 'BLDP': 'Energy', 'FLNC': 'Energy',
            
            # Real Estate
            'EXR': 'Real Estate', 'CBRE': 'Real Estate', 'SBAC': 'Real Estate',
            'CCI': 'Real Estate', 'AMT': 'Real Estate', 'DLR': 'Real Estate',
            'EQIX': 'Real Estate', 'PSA': 'Real Estate',
            
            # International/ADRs
            'MELI': 'International', 'JD': 'International', 'PDD': 'International',
            'BIDU': 'International', 'VIPS': 'International', 'KC': 'International',
            'TIGR': 'International', 'FUTU': 'International', 'NIU': 'International',
            
            # Special/Other
            'RBLX': 'Gaming', 'U': 'Technology'
        }
        
        return sector_mapping.get(symbol, 'Other')
    
    def _get_industry(self, symbol: str) -> str:
        """Get industry for symbol (simplified)"""
        return f"{self._get_sector(symbol)}_Industry"